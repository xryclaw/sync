# SLS æ—¥å¿—åˆ†æå¹³å° - MVP å®æ–½è®¡åˆ’

> **For Claude:** æœ¬è®¡åˆ’åŸºäº writing-plans æŠ€èƒ½åˆ›å»ºï¼Œç”¨äºæŒ‡å¯¼ MVP ç‰ˆæœ¬çš„é€æ­¥å®æ–½ã€‚

**ç›®æ ‡ï¼š** æ„å»ºä¸€ä¸ªæç®€çš„æ—¥å¿—åˆ†æå·¥å…·ï¼Œæ”¯æŒ CSV ä¸Šä¼ ã€æ—¥å¿—æŸ¥è¯¢ã€è¡¨æ ¼å±•ç¤ºå’Œç­›é€‰åŠŸèƒ½ã€‚

**æ¶æ„ï¼š** åç«¯ä½¿ç”¨ Node.js + Express + SQLiteï¼›å‰ç«¯ä½¿ç”¨ React + Vite + Ant Designï¼›æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ Dockerã€‚

**æŠ€æœ¯æ ˆï¼š**
- åç«¯ï¼šNode.js 20 + Express 4 + SQLite3 + Multer + csv-parser
- å‰ç«¯ï¼šReact 18 + Vite + Ant Design 5 + Axios
- æ•°æ®åº“ï¼šSQLite3ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰

---

## å®æ–½ä»»åŠ¡æ¸…å•

### âœ… Task 0: è®¾è®¡æ–‡æ¡£ï¼ˆå·²å®Œæˆï¼‰
### ğŸ“ Task 1: é¡¹ç›®åˆå§‹åŒ–
### ğŸ“ Task 2: åç«¯æ•°æ®åº“è®¾è®¡
### ğŸ“ Task 3: CSV ä¸Šä¼ åŠŸèƒ½
### ğŸ“ Task 4: æ—¥å¿—æŸ¥è¯¢ API
### ğŸ“ Task 5: å‰ç«¯é¡¹ç›®åˆå§‹åŒ–
### ğŸ“ Task 6: ä¸Šä¼ é¡µé¢
### ğŸ“ Task 7: æ—¥å¿—æŸ¥çœ‹é¡µé¢
### ğŸ“ Task 8: æµ‹è¯•ä¸ä¼˜åŒ–

---

## Task 1: é¡¹ç›®åˆå§‹åŒ–

**Files:**
- Create: `sls-log-analyzer/README.md`
- Create: `sls-log-analyzer/.gitignore`
- Create: `sls-log-analyzer/backend/package.json`
- Create: `sls-log-analyzer/frontend/package.json`

**Step 1: åˆ›å»ºé¡¹ç›®æ ¹ç›®å½•**

```bash
mkdir sls-log-analyzer
cd sls-log-analyzer
```

**Step 2: åˆå§‹åŒ– Git ä»“åº“**

```bash
git init
```

**Step 3: åˆ›å»º .gitignore**

```gitignore
# Dependencies
node_modules/
npm-debug.log*

# Database
*.db
*.sqlite
*.sqlite3

# Uploads
uploads/
temp/

# Environment
.env
.env.local

# Build
dist/
build/

# IDE
.vscode/
.idea/

# OS
.DS_Store
Thumbs.db
```

**Step 4: åˆ›å»º README.md**

```markdown
# SLS æ—¥å¿—åˆ†æå¹³å° - MVP

## åŠŸèƒ½
- CSV æ—¥å¿—æ–‡ä»¶ä¸Šä¼ 
- æ—¥å¿—è¡¨æ ¼å±•ç¤º
- æ—¶é—´èŒƒå›´ç­›é€‰
- æ—¥å¿—çº§åˆ«ç­›é€‰ï¼ˆInfo/Errorï¼‰
- æŸ¥çœ‹åŸå§‹æ—¥å¿—è¯¦æƒ…

## æŠ€æœ¯æ ˆ
- åç«¯ï¼šNode.js + Express + SQLite
- å‰ç«¯ï¼šReact + Ant Design

## å¿«é€Ÿå¼€å§‹

### åç«¯
```bash
cd backend
npm install
npm run dev
```

### å‰ç«¯
```bash
cd frontend
npm install
npm run dev
```

è®¿é—®ï¼šhttp://localhost:3000
```

**Step 5: åˆ›å»ºåç«¯ç›®å½•ç»“æ„**

```bash
mkdir -p backend/src/routes
mkdir -p backend/src/services
mkdir -p backend/database
mkdir -p backend/uploads
```

**Step 6: åˆå§‹åŒ–åç«¯ package.json**

```bash
cd backend
npm init -y
```

ä¿®æ”¹ `backend/package.json`ï¼š

```json
{
  "name": "sls-log-analyzer-backend",
  "version": "1.0.0",
  "description": "SLS æ—¥å¿—åˆ†æå¹³å°åç«¯",
  "main": "src/app.js",
  "scripts": {
    "start": "node src/app.js",
    "dev": "nodemon src/app.js"
  },
  "dependencies": {
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "multer": "^1.4.5-lts.1",
    "csv-parser": "^3.0.0",
    "sqlite3": "^5.1.6",
    "dotenv": "^16.3.1"
  },
  "devDependencies": {
    "nodemon": "^3.0.2"
  }
}
```

**Step 7: å®‰è£…åç«¯ä¾èµ–**

```bash
npm install
```

**Step 8: åˆ›å»ºå‰ç«¯é¡¹ç›®**

```bash
cd ..
npm create vite@latest frontend -- --template react
cd frontend
npm install
```

**Step 9: å®‰è£…å‰ç«¯ä¾èµ–**

```bash
npm install antd axios react-router-dom dayjs
```

**Step 10: æäº¤åˆå§‹åŒ–**

```bash
cd ..
git add .
git commit -m "chore: é¡¹ç›®åˆå§‹åŒ–"
```

---

## Task 2: åç«¯æ•°æ®åº“è®¾è®¡

**Files:**
- Create: `backend/src/db.js`
- Create: `backend/.env.example`

**Step 1: åˆ›å»º .env.example**

```env
PORT=5000
NODE_ENV=development
DATABASE_PATH=./database/logs.db
UPLOAD_DIR=./uploads
MAX_FILE_SIZE=52428800
```

**Step 2: åˆ›å»ºæ•°æ®åº“åˆå§‹åŒ–æ–‡ä»¶**

åˆ›å»º `backend/src/db.js`ï¼š

```javascript
const sqlite3 = require('sqlite3').verbose();
const path = require('path');
const fs = require('fs');

const dbDir = path.join(__dirname, '../database');
if (!fs.existsSync(dbDir)) {
  fs.mkdirSync(dbDir, { recursive: true });
}

const dbPath = process.env.DATABASE_PATH || path.join(dbDir, 'logs.db');

const db = new sqlite3.Database(dbPath, (err) => {
  if (err) {
    console.error('æ•°æ®åº“è¿æ¥å¤±è´¥:', err.message);
    process.exit(1);
  } else {
    console.log('SQLite æ•°æ®åº“è¿æ¥æˆåŠŸ:', dbPath);
    initDatabase();
  }
});

function initDatabase() {
  db.serialize(() => {
    // æ—¥å¿—è¡¨
    db.run(`
      CREATE TABLE IF NOT EXISTS logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        session_id INTEGER,
        uid TEXT,
        datetime TEXT NOT NULL,
        level TEXT,
        resp TEXT,
        sid TEXT,
        device TEXT,
        client_ver TEXT,
        pack_ver TEXT,
        country TEXT,
        store TEXT,
        raw_json TEXT,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
      )
    `);

    // ç´¢å¼•
    db.run('CREATE INDEX IF NOT EXISTS idx_datetime ON logs(datetime)');
    db.run('CREATE INDEX IF NOT EXISTS idx_level ON logs(level)');
    db.run('CREATE INDEX IF NOT EXISTS idx_session ON logs(session_id)');

    // ä¸Šä¼ ä¼šè¯è¡¨
    db.run(`
      CREATE TABLE IF NOT EXISTS sessions (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        file_name TEXT,
        upload_time DATETIME DEFAULT CURRENT_TIMESTAMP,
        total_logs INTEGER,
        error_count INTEGER,
        uid TEXT
      )
    `);

    console.log('æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ');
  });
}

module.exports = db;
```

**Step 3: æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–**

åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶ `backend/test-db.js`ï¼š

```javascript
require('dotenv').config();
const db = require('./src/db');

setTimeout(() => {
  db.all('SELECT name FROM sqlite_master WHERE type="table"', [], (err, rows) => {
    if (err) {
      console.error('æŸ¥è¯¢å¤±è´¥:', err);
    } else {
      console.log('æ•°æ®åº“è¡¨:', rows);
    }
    db.close();
  });
}, 1000);
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
node test-db.js
```

é¢„æœŸè¾“å‡ºï¼š
```
SQLite æ•°æ®åº“è¿æ¥æˆåŠŸ: ./database/logs.db
æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ
æ•°æ®åº“è¡¨: [ { name: 'logs' }, { name: 'sessions' } ]
```

**Step 4: åˆ é™¤æµ‹è¯•æ–‡ä»¶**

```bash
rm test-db.js
```

**Step 5: æäº¤æ•°æ®åº“è®¾è®¡**

```bash
git add .
git commit -m "feat: æ•°æ®åº“è®¾è®¡ä¸åˆå§‹åŒ–"
```

---

## Task 3: CSV ä¸Šä¼ åŠŸèƒ½

**Files:**
- Create: `backend/src/services/csvParser.js`
- Create: `backend/src/routes/upload.js`
- Create: `backend/src/app.js`

**Step 1: åˆ›å»º CSV è§£ææœåŠ¡**

åˆ›å»º `backend/src/services/csvParser.js`ï¼š

```javascript
const fs = require('fs');
const csv = require('csv-parser');
const db = require('../db');

class CSVParser {
  static async parseAndStore(filePath, fileName) {
    return new Promise((resolve, reject) => {
      const logs = [];
      let errorCount = 0;
      let uid = null;

      fs.createReadStream(filePath)
        .pipe(csv())
        .on('data', (row) => {
          try {
            // æå– uidï¼ˆå–ç¬¬ä¸€æ¡çš„ uidï¼‰
            if (!uid && row.uid) {
              uid = row.uid;
            }

            // ç»Ÿè®¡ error
            if (row._level_ && row._level_.toLowerCase() === 'error') {
              errorCount++;
            }

            logs.push({
              uid: row.uid || '',
              datetime: row._datetime_ || '',
              level: row._level_ || 'Info',
              resp: row.resp || '',
              sid: row.sid || '',
              device: row.device || '',
              client_ver: row.clientVer || '',
              pack_ver: row.packVer || '',
              country: row.country || '',
              store: row.store || '',
              raw_json: JSON.stringify(row)
            });
          } catch (err) {
            console.error('è§£æè¡Œå¤±è´¥:', err.message);
          }
        })
        .on('end', async () => {
          try {
            // åˆ›å»ºä¼šè¯
            const sessionId = await this.createSession(fileName, logs.length, errorCount, uid);
            
            // æ‰¹é‡æ’å…¥æ—¥å¿—
            await this.batchInsertLogs(logs, sessionId);
            
            resolve({
              sessionId,
              totalLogs: logs.length,
              errorCount,
              uid
            });
          } catch (err) {
            reject(err);
          }
        })
        .on('error', reject);
    });
  }

  static createSession(fileName, totalLogs, errorCount, uid) {
    return new Promise((resolve, reject) => {
      const sql = `
        INSERT INTO sessions (file_name, total_logs, error_count, uid)
        VALUES (?, ?, ?, ?)
      `;
      
      db.run(sql, [fileName, totalLogs, errorCount, uid], function(err) {
        if (err) reject(err);
        else resolve(this.lastID);
      });
    });
  }

  static batchInsertLogs(logs, sessionId) {
    return new Promise((resolve, reject) => {
      const sql = `
        INSERT INTO logs (session_id, uid, datetime, level, resp, sid, device, client_ver, pack_ver, country, store, raw_json)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
      `;
      
      const stmt = db.prepare(sql);
      
      db.serialize(() => {
        db.run('BEGIN TRANSACTION');
        
        logs.forEach(log => {
          stmt.run([
            sessionId,
            log.uid,
            log.datetime,
            log.level,
            log.resp,
            log.sid,
            log.device,
            log.client_ver,
            log.pack_ver,
            log.country,
            log.store,
            log.raw_json
          ]);
        });
        
        stmt.finalize();
        db.run('COMMIT', (err) => {
          if (err) reject(err);
          else resolve();
        });
      });
    });
  }
}

module.exports = CSVParser;
```

**Step 2: åˆ›å»ºä¸Šä¼ è·¯ç”±**

åˆ›å»º `backend/src/routes/upload.js`ï¼š

```javascript
const express = require('express');
const router = express.Router();
const multer = require('multer');
const path = require('path');
const fs = require('fs');
const CSVParser = require('../services/csvParser');

const uploadDir = process.env.UPLOAD_DIR || './uploads';
if (!fs.existsSync(uploadDir)) {
  fs.mkdirSync(uploadDir, { recursive: true });
}

const storage = multer.diskStorage({
  destination: (req, file, cb) => {
    cb(null, uploadDir);
  },
  filename: (req, file, cb) => {
    const uniqueSuffix = Date.now() + '-' + Math.round(Math.random() * 1E9);
    cb(null, 'upload-' + uniqueSuffix + path.extname(file.originalname));
  }
});

const upload = multer({
  storage: storage,
  fileFilter: (req, file, cb) => {
    if (file.mimetype === 'text/csv' || path.extname(file.originalname) === '.csv') {
      cb(null, true);
    } else {
      cb(new Error('åªæ”¯æŒ CSV æ–‡ä»¶'), false);
    }
  },
  limits: {
    fileSize: parseInt(process.env.MAX_FILE_SIZE) || 50 * 1024 * 1024
  }
});

router.post('/', upload.single('file'), async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'æœªä¸Šä¼ æ–‡ä»¶' });
    }

    console.log('å¼€å§‹è§£ææ–‡ä»¶:', req.file.originalname);
    
    const result = await CSVParser.parseAndStore(req.file.path, req.file.originalname);
    
    // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    fs.unlinkSync(req.file.path);
    
    res.json({
      success: true,
      message: 'æ–‡ä»¶ä¸Šä¼ å¹¶è§£ææˆåŠŸ',
      data: result
    });
  } catch (error) {
    console.error('ä¸Šä¼ å¤„ç†å¤±è´¥:', error);
    
    if (req.file && fs.existsSync(req.file.path)) {
      fs.unlinkSync(req.file.path);
    }
    
    res.status(500).json({ 
      error: 'æ–‡ä»¶å¤„ç†å¤±è´¥', 
      message: error.message 
    });
  }
});

router.get('/sessions', (req, res) => {
  const db = require('../db');
  
  db.all('SELECT * FROM sessions ORDER BY upload_time DESC', [], (err, rows) => {
    if (err) {
      return res.status(500).json({ error: 'æŸ¥è¯¢å¤±è´¥', message: err.message });
    }
    res.json({ success: true, data: rows });
  });
});

module.exports = router;
```

**Step 3: åˆ›å»ºä¸»åº”ç”¨æ–‡ä»¶**

åˆ›å»º `backend/src/app.js`ï¼š

```javascript
require('dotenv').config();
const express = require('express');
const cors = require('cors');
const path = require('path');

const app = express();
const PORT = process.env.PORT || 5000;

// ä¸­é—´ä»¶
app.use(cors());
app.use(express.json());
app.use(express.urlencoded({ extended: true }));

// å¥åº·æ£€æŸ¥
app.get('/health', (req, res) => {
  res.json({ status: 'ok', timestamp: new Date().toISOString() });
});

// è·¯ç”±
app.use('/api/upload', require('./routes/upload'));

// é”™è¯¯å¤„ç†
app.use((err, req, res, next) => {
  console.error(err.stack);
  res.status(500).json({ 
    error: 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯', 
    message: process.env.NODE_ENV === 'development' ? err.message : undefined 
  });
});

app.listen(PORT, () => {
  console.log(`åç«¯æœåŠ¡è¿è¡Œåœ¨ http://localhost:${PORT}`);
});
```

**Step 4: åˆ›å»º .env æ–‡ä»¶**

```bash
cp .env.example .env
```

**Step 5: æµ‹è¯•åç«¯å¯åŠ¨**

```bash
npm run dev
```

é¢„æœŸè¾“å‡ºï¼š
```
SQLite æ•°æ®åº“è¿æ¥æˆåŠŸ: ./database/logs.db
æ•°æ®åº“è¡¨åˆå§‹åŒ–å®Œæˆ
åç«¯æœåŠ¡è¿è¡Œåœ¨ http://localhost:5000
```

**Step 6: æäº¤ä¸Šä¼ åŠŸèƒ½**

```bash
git add .
git commit -m "feat: CSV ä¸Šä¼ åŠŸèƒ½"
```

---

